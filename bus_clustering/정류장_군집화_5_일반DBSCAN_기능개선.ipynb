{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1 정류장 파일 읽어온 뒤, 'city(시)' 필드 추가\n",
    "#### 1-2 제주시, 서귀포시 정류장 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = r'C:\\Users\\think\\Desktop\\버스 정류장 클러스터링'.replace('\\\\', '/')\n",
    "path = r'D:\\jeju_bus_data_no_leakage\\station'.replace('\\\\', '/')\n",
    "df = pd.read_csv(path + '/station_final.csv', encoding = 'ansi')\n",
    "df['city'] = df['STATION_ADDR']\n",
    "df.loc[:, 'city'] = df.loc[:, 'STATION_ADDR'].apply(lambda string: string.split(' ')[0])\n",
    "                    # (예) 'STATION_ADDR': 서귀포시 중문동 ... ☞ 'city': 서귀포시\n",
    "\n",
    "df_jeju_station     = df.query('city == \"제주시\"')\n",
    "df_seogwipo_station = df.query('city == \"서귀포시\"')\n",
    "\n",
    "list_station_df_per_city = [df_jeju_station, df_seogwipo_station]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_s = 'target'\n",
    "level_s = 'level'\n",
    "earth_radius = 6371.0088 # 단위: km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_level_spatial_dbscan_result \\\n",
    "    (df, n, eps, min_pts = 3, nonf_cols = ['STATION_ID', 'STATION_NM'], f_cols = ['LOCAL_X', 'LOCAL_Y']):\n",
    "    global earth_radius \n",
    "    dbscan = DBSCAN(eps = eps/1000/earth_radius, algorithm='ball_tree', \n",
    "                    metric='haversine', min_samples=min_pts)\n",
    "    \n",
    "    temp_df = df.loc[:, nonf_cols + f_cols]\n",
    "    temp_df.loc[:, target_s] = dbscan.fit_predict(np.radians(temp_df[f_cols]))\n",
    "\n",
    "    success_index = temp_df.query(target_s + ' > -1').index\n",
    "    temp_df.loc[success_index, level_s] = str(n)\n",
    "    \n",
    "    failed_index = set(temp_df.index) - set(success_index)\n",
    "    temp_df.loc[failed_index, level_s] = str(-1)\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_handled_result \\\n",
    "    (df, n, by1='STATION_ID', by2='STATION_NM', f_cols=['LOCAL_X', 'LOCAL_Y']):\n",
    "    temp_df = df.loc[:, [by1, by2] + f_cols]\n",
    "\n",
    "    temp_df.loc[:, 'target'] = -1\n",
    "    temp_df.loc[:, 'level'] = -1\n",
    "\n",
    "    grouped_df = temp_df.groupby(by = by2, as_index = False)\n",
    "    grouped_df_count = grouped_df.count()\n",
    "    station_nm_list = list(grouped_df_count[grouped_df_count[by1] >= 2][by2])\n",
    "    target_range = np.arange(len(station_nm_list))\n",
    "\n",
    "    for i in target_range:\n",
    "        list_idx = temp_df.query('%s == \"%s\"' % (by2, station_nm_list[i])).index\n",
    "        for idx in list_idx:\n",
    "            temp_df.loc[idx, 'target'] = i\n",
    "            temp_df.loc[idx, 'level'] = n\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_result(df, n):\n",
    "    df.loc[:, 'level'] = n\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_dbscan_result(eps, station_df):\n",
    "    df_lv1_group = get_n_level_spatial_dbscan_result(station_df, 1, eps, min_pts = 3)\n",
    "    \n",
    "    df_lv2_group = df_lv1_group.query(target_s + ' == -1')\n",
    "    df_lv2_group = get_n_level_spatial_dbscan_result(df_lv2_group, 2, eps, min_pts = 2)\n",
    "    \n",
    "    df_noise_handled_group = df_lv2_group.query(target_s + ' == -1')\n",
    "    df_noise_handled_group = get_noise_handled_result(df_noise_handled_group, 3)\n",
    "    \n",
    "    df_noise_group = df_noise_handled_group.query(target_s + ' == -1')\n",
    "    df_noise_group = get_noise_result(df_noise_group, 4)\n",
    "    \n",
    "    r1 = df_lv1_group.query(target_s + ' > -1')\n",
    "    r2 = df_lv2_group.query(target_s + ' > -1')\n",
    "    r3 = df_noise_handled_group.query(target_s + ' > -1')\n",
    "    r4 = df_noise_group\n",
    "    \n",
    "    combined = pd.concat([r1, r2, r3, r4])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 정류장 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#list_station_cnt_per_group_per_level_per_eps_per_city = []\n",
    "eps_list = [110, 95]\n",
    "list_df_combined = []\n",
    "\n",
    "for i, station_df in enumerate(list_station_df_per_city):\n",
    "    #list_station_cnt_per_group_per_level_per_eps_per_city.append(pd.DataFrame(columns=[1., 2., 3., 4.]))\n",
    "    \n",
    "    eps = eps_list[i]\n",
    "    # eps에 따른 dbscan 수행.\n",
    "    df_combined = get_spatial_dbscan_result(eps, station_df)\n",
    "    df_combined.loc[:, 'level'] = df_combined['level'].astype(str)\n",
    "    df_combined.loc[:, 'target'] = df_combined['target'].astype(str)\n",
    "    df_combined['level-target'] = df_combined['level'] + '&' + df_combined['target']\n",
    "\n",
    "    # lv별 grouping -> count -> 저장.\n",
    "    list_df_combined.append(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_combined[0].to_csv('jeju_city_clustered.csv', index = False, encoding = 'ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_combined[1].to_csv('seogwipo_city_clustered.csv', index = False, encoding = 'ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
